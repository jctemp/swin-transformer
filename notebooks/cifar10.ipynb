{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as callbacks\n",
    "import lightning.pytorch.loggers as loggers\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "from model import SwinTransformer2D, SwinTransformerConfig2D\n",
    "from notebooks.utils.configs import MODEL_CONFIGS\n",
    "from notebooks.utils.reference import SwinTransformerReference2D\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./notebooks/cifar10\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "LOGS_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "CKPT_DIR = os.path.join(BASE_DIR, \"ckpts\")\n",
    "\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"SKIP\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n",
    "train_dataset, val_dataset = random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "train_data = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "val_data = torch.stack([img_t for img_t, _ in val_dataset], dim=3)\n",
    "test_data = torch.stack([img_t for img_t, _ in test_dataset], dim=3)\n",
    "\n",
    "train_mean, train_std = train_data.view(3, -1).mean(dim=1), train_data.view(3, -1).std(dim=1)\n",
    "val_mean, val_std = val_data.view(3, -1).mean(dim=1), val_data.view(3, -1).std(dim=1)\n",
    "test_mean, test_std = test_data.view(3, -1).mean(dim=1), test_data.view(3, -1).std(dim=1)\n",
    "\n",
    "print(f\"Train mean: {train_mean}, std: {train_std}\")\n",
    "print(f\"Val mean: {val_mean}, std: {val_std}\")\n",
    "print(f\"Test mean: {test_mean}, std: {test_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_mean = (0.4912, 0.4820, 0.4464)\n",
    "train_std = (0.2472, 0.2436, 0.2617)\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(train_mean, train_std),\n",
    "        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_train)\n",
    "train_dataset, _ = random_split(train_dataset, [45000, 5000])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "val_mean = (0.4932, 0.4836, 0.4474)\n",
    "val_std = (0.2459, 0.2422, 0.2608)\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(val_mean, val_std),\n",
    "    ]\n",
    ")\n",
    "val_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_val)\n",
    "_, val_dataset = random_split(val_dataset, [45000, 5000])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "\n",
    "test_mean = (0.4942, 0.4851, 0.4504)\n",
    "test_std = (0.2467, 0.2429, 0.2616)    \n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(test_mean, test_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.swin = SwinTransformer2D(config)\n",
    "        self.norm = nn.LayerNorm(self.swin.out_channels[-1])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.fc = nn.Linear(self.swin.out_channels[-1], 10)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.swin(x)\n",
    "        x = out[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerClf(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, name=\"default\", config=None, max_epochs=100, steps_per_epoch=300, learning_rate=3e-4, weight_decay=0.05\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "\n",
    "        if config is None:\n",
    "            self.model = nn.Identity()\n",
    "        else:\n",
    "            try:\n",
    "                config = SwinTransformerConfig2D(**config) if isinstance(config, dict) else config\n",
    "                self.model = SwinModel(config)\n",
    "            except Exception:\n",
    "                self.model = SwinTransformerReference2D(**config)\n",
    "\n",
    "        self.save_hyperparameters(ignore=\"model\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nnf.softmax(self.model(x), dim=1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"train_acc\": acc},\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": loss, \"val_acc\": acc},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"test_loss\": loss, \"test_acc\": acc},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.learning_rate, epochs=self.max_epochs, steps_per_epoch=self.steps_per_epoch\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    name: SwinTransformerClf(name, config, num_epochs, len(train_loader), learning_rate, weight_decay)\n",
    "    for name, config in MODEL_CONFIGS.items()\n",
    "}\n",
    "\n",
    "for clf in models:\n",
    "    print(summary(models[clf], (batch_size, 3, 32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in models.items():\n",
    "    print(\"=\" * 80, flush=True)\n",
    "    print(f\"Training model {name}\", flush=True)\n",
    "    print()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    csv_logger = loggers.CSVLogger(LOGS_DIR, name=clf.name)\n",
    "    learning_rate_monitor = callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_acc\",\n",
    "        dirpath=CKPT_DIR,\n",
    "        filename=clf.name,\n",
    "        save_top_k=1,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        logger=csv_logger,\n",
    "        callbacks=[learning_rate_monitor, model_checkpoint],\n",
    "        gradient_clip_val=1.0,\n",
    "        precision=\"16-mixed\",\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80, flush=True)\n",
    "    \n",
    "    trainer.fit(model=clf, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(model=clf, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n",
    "tra_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logs = [os.path.join(LOGS_DIR, f) for f in os.listdir(LOGS_DIR)]\n",
    "model_logs = [[os.path.join(f, v) for v in os.listdir(f)] for f in model_logs]\n",
    "model_logs = [max(v, key=os.path.getctime) for v in model_logs]\n",
    "model_logs = [os.path.join(v, \"metrics.csv\") for v in model_logs]\n",
    "model_logs.sort()\n",
    "\n",
    "ckpt_files = os.listdir(CKPT_DIR)\n",
    "ckpt_files = [f for f in ckpt_files if f.endswith(\".ckpt\")]\n",
    "\n",
    "models = {}\n",
    "for ckpt in ckpt_files:\n",
    "    try:\n",
    "        model_name = ckpt.split(\".\")[0]\n",
    "        clf = SwinTransformerClf.load_from_checkpoint(os.path.join(CKPT_DIR, ckpt))\n",
    "        models[model_name] = clf\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "models = {k: v for k, v in sorted(models.items(), key=lambda item: item[1].name)}\n",
    "\n",
    "model_logs, [m.name for m in models.values()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(12, 12), dpi=100)\n",
    "fig.suptitle(\"Training, Validation, and Test Accuracy over Epochs\")\n",
    "\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax_acc = fig.add_subplot(gs[0, 0])\n",
    "ax_loss = fig.add_subplot(gs[0, 1])\n",
    "ax_test = fig.add_subplot(gs[1, :])\n",
    "\n",
    "max_acc = 0\n",
    "for i, (logs, clf) in enumerate(zip(model_logs, models.values())):\n",
    "    df = pd.read_csv(logs)\n",
    "    col = list(mcolors.BASE_COLORS.keys())[i]\n",
    "\n",
    "    if \"train_acc_epoch\" in df.columns:\n",
    "        train_data = df[df[\"train_acc_epoch\"].notna()]\n",
    "        val_data = df[df[\"val_acc\"].notna()]\n",
    "        ax_acc.plot(\n",
    "            train_data[\"epoch\"],\n",
    "            train_data[\"train_acc_epoch\"],\n",
    "            label=f\"Training Accuracy {clf.name}\",\n",
    "            color=col,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        ax_acc.plot(\n",
    "            val_data[\"epoch\"],\n",
    "            val_data[\"val_acc\"],\n",
    "            label=f\"Validation Accuracy {clf.name}\",\n",
    "            color=col,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "    ax_acc.set_xlabel(\"Epoch\")\n",
    "    ax_acc.set_ylabel(\"Accuracy\")\n",
    "    ax_acc.legend()\n",
    "    ax_acc.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    if \"train_loss_epoch\" in df.columns:\n",
    "        train_data = df[df[\"train_loss_epoch\"].notna()]\n",
    "        val_data = df[df[\"val_loss\"].notna()]\n",
    "        ax_loss.plot(\n",
    "            train_data[\"epoch\"],\n",
    "            train_data[\"train_loss_epoch\"],\n",
    "            label=f\"Training Loss {clf.name}\",\n",
    "            color=col,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        ax_loss.plot(\n",
    "            val_data[\"epoch\"],\n",
    "            val_data[\"val_loss\"],\n",
    "            label=f\"Validation Loss {clf.name}\",\n",
    "            color=col,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Loss\")\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    if \"test_acc\" in df.columns:\n",
    "        test_data = df[df[\"test_acc\"].notna()]\n",
    "        ax_test.barh(i, test_data[\"test_acc\"], label=f\"Test Accuracy {clf.name}\")\n",
    "        ax_test.set_xlabel(\"Epoch\")\n",
    "        ax_test.set_ylabel(\"Accuracy\")\n",
    "        ax_test.legend()\n",
    "        ax_test.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        if test_data[\"test_acc\"].max() > max_acc:\n",
    "            max_acc = test_data[\"test_acc\"].max()\n",
    "\n",
    "ax_test.axvline(max_acc, color=\"black\", linestyle=\"--\")\n",
    "ax_test.text(max_acc + 0.005, i, f\"{max_acc:.2f}\", va=\"center\", ha=\"left\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "image = ref_dataset[idx][0].permute(1, 2, 0)\n",
    "label = ref_dataset[idx][1]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(LABELS[label])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "image_model = tra_dataset[idx][0]\n",
    "for clf in models.values():\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = clf(image_model.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in models.items():\n",
    "    if isinstance(clf.model, SwinModel):\n",
    "        print(name)\n",
    "        rel_embed_table = clf.model.swin.stages[0].blocks[0].attn.attn_weights.sum(dim=1)\n",
    "        count = rel_embed_table.shape[1]\n",
    "        split = int(math.sqrt(count))\n",
    "\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(12, 12), dpi=100)\n",
    "        fig.suptitle(\"Attention Weights\")\n",
    "\n",
    "        gs = fig.add_gridspec(split, split)\n",
    "        for i in range(count):\n",
    "            ax = fig.add_subplot(gs[i // split, i % split])\n",
    "            ax.set_title(f\"Window {i}\")\n",
    "            ax.imshow(rel_embed_table[i].detach().cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in models.items():\n",
    "    if isinstance(clf.model, SwinModel):\n",
    "        print(name, flush=True)\n",
    "\n",
    "        cnt = 0\n",
    "        if clf.model.swin.stages[0].blocks[0].attn.bias_mode:\n",
    "            rel_embed_table = clf.model.swin.stages[0].blocks[0].attn.embedding_table\n",
    "            factor = 2\n",
    "            cnt = 1\n",
    "        elif clf.model.swin.stages[0].blocks[0].attn.context_mode:\n",
    "            rel_embed_table_q = clf.model.swin.stages[0].blocks[0].attn.embedding_table_q\n",
    "            rel_embed_table_k = clf.model.swin.stages[0].blocks[0].attn.embedding_table_k\n",
    "            factor = 10\n",
    "            cnt = 2\n",
    "\n",
    "        if cnt == 0:\n",
    "            continue\n",
    "\n",
    "        ratio = rel_embed_table.weight.shape[0] / rel_embed_table.weight.shape[1]\n",
    "\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(ratio * factor, factor), dpi=100)\n",
    "        fig.suptitle(\"Relative Positional Embeddings\")\n",
    "        gs = fig.add_gridspec(cnt, 1)\n",
    "\n",
    "        if cnt == 1:\n",
    "            ax = fig.add_subplot(gs[0, 0])\n",
    "            ax.imshow(rel_embed_table.weight.transpose(0,1).detach().cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_ylabel(\"Embedding Dimension\")\n",
    "            ax.set_xlabel(\"Relative Position\")\n",
    "\n",
    "        elif cnt == 2:\n",
    "            ax = fig.add_subplot(gs[0, 0])\n",
    "            ax.imshow(rel_embed_table_q.weight.detach().cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel(\"Embedding Dimension\")\n",
    "            ax.set_ylabel(\"Relative Position\")\n",
    "            \n",
    "            ax = fig.add_subplot(gs[1, 0])\n",
    "            ax.imshow(rel_embed_table_k.weight.detach().cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel(\"Embedding Dimension\")\n",
    "            ax.set_ylabel(\"Relative Position\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
