{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import einops.layers.torch as elt\n",
    "import einops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (12, 10)\n",
    "fontsize_figure = 16\n",
    "fontsize_plot = 12\n",
    "fontsize_axis = 10\n",
    "dpi=75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patch embedding and the patch merge modules are almost identical. The original Swin Transformer implementation made\n",
    "a difference regarding the implementation. For the embedding process, they utilised a 2d convolution layer. For  \n",
    "patch merging, they concatenated a patch along the embedding dimension. Afterwards, a linear projection\n",
    "is applied to get the new embedding dimension.\n",
    "\n",
    "![patch operation](../images/patch-operation.png)\n",
    "\n",
    "Nonetheless, technically, one can employ both strategies depending on the problem. The difference is the number of\n",
    "learnable parameters. Consider the following example:\n",
    "\n",
    "- in_channels = $128$, out_channels = $64$, kernel_size = $(2, 2)$\n",
    "- param_linr = $128 \\cdot 64 + 64 = 8256$\n",
    "- param_conv = $128 \\cdot 64 \\cdot (2 \\cdot 2) + 64 = 32832$\n",
    "\n",
    "As one can see, the parameter count differs significantly. However, the additional parameter of a convolutional layer\n",
    "might help a model to learn the spatial relationship within a patch and provide a more sophisticated representation.\n",
    "The most challenging part is the rearrangement of multi-dimensional tensors (at least for me). First, we require the\n",
    "correct torch function to rearrange and concatenate the tensors or perform clever slicing. Alternatively, one \n",
    "imports a library called _einops_, which allows us to perform multi-dimensional tensor manipulation in a readable\n",
    "format. Regardless, once implemented, one uses it throughout the project.\n",
    "\n",
    "```py\n",
    "# With PyTorch native\n",
    "b, c, h, w = image.shape\n",
    "hm, wm = 2, 2\n",
    "hdm, wdm = h // hm, w // wm\n",
    "image = image # (b, c, h, w)\n",
    "image = image.unfold(-2, hm, hm)  # (b, c, hdm, w, hm)\n",
    "image = image.unfold(-2, wm, wm)  # (b, c, hdm, wdm, hm, wm)\n",
    "image = image.reshape(b, hdm, wdm, c * hm * wm)\n",
    "\n",
    "# Or with Einops\n",
    "from einops import rearrange\n",
    "image = rearrange(image, \"b c (hdm hm) (wdm wm) -> b hdm wdm (c hm wm)\", hm=hm, wm=wm)\n",
    "\n",
    "# Or even TorchScript compatible\n",
    "from einops.layers.torch import Rearrange\n",
    "image = Rearrange(\"b c (hdm hm) (wdm wm) -> b hdm wdm (c hm wm)\", hm=hm, wm=wm)(image)\n",
    "```\n",
    "\n",
    "<style type=\"text/css\">img { width: 80ch; }</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters of a layer\n",
    "h, w = (6, 8)\n",
    "hm, wm = (2, 2)\n",
    "c = 3 or 1\n",
    "c_embed = 12\n",
    "hdm, wdm = h // hm, w // wm\n",
    "\n",
    "# Input can be an image or a sequence\n",
    "image_spt = torch.randn(1, c, h, w)\n",
    "image_spt = image_spt / image_spt.max()\n",
    "image_seq = einops.rearrange(image_spt, \"b c h w -> b (h w) c\")\n",
    "\n",
    "print(\"Input\")\n",
    "print(f\"'b c h w'   = {tuple(image_spt.shape)}\")\n",
    "print(f\"'b (h w) c' = {tuple(image_seq.shape)}\")\n",
    "print()\n",
    "\n",
    "# The rearrange operations and the projection\n",
    "# Note: the convolutional projection is basically a window which we shift by\n",
    "#       its size to get the patches\n",
    "rearrange_input_spt = elt.Rearrange(\"b c h w -> b c h w\", h=h, w=w)\n",
    "rearrange_input_seq = elt.Rearrange(\"b (h w) c -> b c h w\", h=h, w=w)\n",
    "rearrange_output = elt.Rearrange(\"b c h w -> b (h w) c\", h=hdm, w=wdm)\n",
    "projection = nn.Conv2d(c, c_embed, kernel_size=(hm, wm), stride=(hm, wm))\n",
    "\n",
    "# For the sake of the visualisation\n",
    "patches = einops.rearrange(image_spt, \"b c (hdm hm) (wdm wm) -> b hdm wdm hm wm c\", hm=hm, wm=wm)\n",
    "\n",
    "# The forward pass\n",
    "patches_projected_spt = projection(rearrange_input_spt(image_spt))\n",
    "patches_projected_seq = projection(rearrange_input_seq(image_seq))\n",
    "\n",
    "print(\"Projection\")\n",
    "print(f\"'b c hdm wdm' = {tuple(patches_projected_spt.shape)}\")\n",
    "print(f\"'b c hdm wdm' = {tuple(patches_projected_seq.shape)}\")\n",
    "print()\n",
    "\n",
    "# Rearrange the output\n",
    "patches_projected_spt_out = rearrange_output(patches_projected_spt)\n",
    "patches_projected_seq_out = rearrange_output(patches_projected_seq)\n",
    "\n",
    "print(\"Output\")\n",
    "print(f\"'b (h w) c' = {tuple(patches_projected_spt_out.shape)}\")\n",
    "print(f\"'b (h w) c' = {tuple(patches_projected_seq_out.shape)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Are the outputs equal? {torch.all(patches_projected_spt_out == patches_projected_seq_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=figsize, dpi=dpi)\n",
    "fig.suptitle(\"Image, Patches, and Projected Patches Visualisation\", fontsize=fontsize_figure)\n",
    "\n",
    "gs = fig.add_gridspec(3, c)\n",
    "\n",
    "for i in range(0, c):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    img = image_spt.squeeze(0)\n",
    "    ax.imshow(img[i].detach().numpy())\n",
    "    ax.set_title(f\"Original Image Channel {i}\", fontsize=fontsize_plot)\n",
    "    ax.add_patch(plt.Rectangle((-0.49, -0.49), wm, hm, edgecolor=\"red\", linewidth=2, facecolor=\"none\"))\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "img_patches = patches.squeeze(0)\n",
    "img_patches = nnf.pad(img_patches, (0, 0, 0, 1, 0, 1), value=torch.nan)\n",
    "img_patches = einops.rearrange(img_patches, \"hdm wdm hm wm c -> (hdm hm) (c wdm wm)\")\n",
    "ax.imshow(img_patches.detach().numpy())\n",
    "ax.set_title(\"Patches\", fontsize=fontsize_plot)\n",
    "ax.axis(\"off\")\n",
    "for i in range(0, c):\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((i * (wm + 1) * wdm - 0.49, -0.49), wm, hm, edgecolor=\"red\", linewidth=2, facecolor=\"none\")\n",
    "    )\n",
    "\n",
    "ax = fig.add_subplot(gs[2, :])\n",
    "ob, on, oc = patches_projected_spt_out.shape\n",
    "img_embed = patches_projected_spt_out.contiguous().view(-1).contiguous()\n",
    "img_embed = nnf.pad(img_embed, (0, on * oc), value=torch.nan).view(2, on * oc)\n",
    "img_embed = einops.rearrange(img_embed, \"k (on oc) -> oc (on k)\", k=2, on=on, oc=oc)\n",
    "ax.imshow(img_embed[:, :-1].detach().numpy())\n",
    "ax.set_title(\"Embedded patches\", fontsize=fontsize_plot)\n",
    "ax.set_xlabel(\"Patch\", fontsize=fontsize_axis)\n",
    "ax.set_ylabel(\"Embedding\", fontsize=fontsize_axis)\n",
    "ax.set_xticks([])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Swin Transformer, as the name suggests, contains self-attention mechanisms\n",
    "that was multi-head self-attention, which was introduced with the _\"Attention is \n",
    "all you need\"_ paper. \n",
    "\n",
    "$$\n",
    "[\\bold{Q},\\bold{K},\\bold{V}] = \\bold{X}\\bold{U} \n",
    "$$\n",
    "$$\n",
    "\\bold{Z} = \\bold{A} \\bold{V}, \\quad\n",
    "\\bold{A} = softmax \\left(\\frac{\\bold{Q}\\bold{K}^\\top}{\\sqrt{d_k}} + \\bold{b}_{rel} \\right)\n",
    "$$\n",
    "\n",
    "One can interpret the equation as a global attention scheme, as all elements in\n",
    "a sequence are attended for each element, leading to the $\\mathcal{O}(n^2)$ runtime. In\n",
    "computer vision, one works with images or volumes that undergo the previously\n",
    "mentioned patching computation. The issue is that selecting a small patch\n",
    "size causes a significant quadratic scaling of the input; for instance, \n",
    "considering a patch size of one would yield an input sequence of $H \\cdot W$.\n",
    "Hence, Liu et al. decided to employ a local attention scheme. They had the idea\n",
    "to group the patch into an $M \\times M$ window. Within this window, one computes the\n",
    "self-attention, limiting the computation to $(M \\times M)^2$. Of course, one has\n",
    "to apply self-attention to each window, yielding $N \\times (M \\times M)^2$ for\n",
    "local and $N \\times N$ for global self-attention. The former is computationally\n",
    "more efficient if the selected $M$ (window size) is significantly smaller than \n",
    "$N$ (number of patches).\n",
    "\n",
    "$$\n",
    "\\frac{H}{M_h} * \\frac{W}{M_w} \\quad where \\quad H, W \\gg M_h, M_w\n",
    "$$\n",
    "\n",
    "As one might have noticed, before computing the attention weight with the softmax,\n",
    "we add a vector of biases $\\bold{b}_{rel}$ to the attention score. The bias is a\n",
    "relative positional embedding. Technically, it is a parametrised relative positional\n",
    "bias for the Swin Transformer, but one can employ different valid embedding\n",
    "strategies depending on the task at hand.\n",
    "\n",
    "![relative positional embedding](../images/relative-positional-embedding-bias.png)\n",
    "\n",
    "One applies self-attention to local windows with the addition of relative\n",
    "positional embeddings. Processing the windows sequentially would be highly\n",
    "inefficient, although a parallel computation is feasible because the windows\n",
    "are disjointed patches. The clever approach is to rearrange the input so that\n",
    "the number of windows becomes the new batch dimension.\n",
    "\n",
    "![window batching](../images/window-batching.png)\n",
    "\n",
    "By batching the windows, we can exploit a framework's parallel processing; see\n",
    "PyTorch or TensorFlow. After the computation, we can rearrange the output to\n",
    "separate the batch dimension again.\n",
    "\n",
    "Recall that the windows are disjointed, and the self-attention is too.\n",
    "Accordingly, to capture intra-window relationships, Liu et al. propose to \n",
    "shift the attention window by $\\lfloor{\\frac{m}{2}}\\rfloor$ where $m$ is the \n",
    "window size. This allows adjacent windows to consider their neighbours in a\n",
    "semantic representation.\n",
    "\n",
    "However, intra-window attention has two problems. The windows at the edges\n",
    "of an image or volume are smaller than ordinary windows. This requires special \n",
    "treatment with techniques like padding, which leads to more windows in total, \n",
    "causing additional computation.\n",
    "Therefore, the author suggested shifting the window in both axes by $\\lfloor{\\frac{m}{2}}\\rfloor$, handling the windows at the edges and keeping the number of windows for an image constant.\n",
    "\n",
    "![window shifting](../images/window-shift.png)\n",
    "\n",
    "Finally, some windows contain elements which are not adjacent (see the illustration of the colours). Therefore, we must perform masking to avoid attention computation of non-adjacent entries. Note that we can pre-compute the mask for efficient processing.\n",
    "\n",
    "<style type=\"text/css\">img { width: 80ch; }</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window batching and shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "hm, wm = 2, 2\n",
    "hdm, wdm = h // hm, w // wm\n",
    "c = 1\n",
    "\n",
    "windows_batching = (\n",
    "    torch.tensor([[0, 0, 1, 1], [0, 0, 1, 1], [2, 2, 3, 3], [2, 2, 3, 3]])\n",
    "    .view(-1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(-1)\n",
    "    .contiguous()\n",
    ")\n",
    "windows_shifting = (\n",
    "    torch.tensor([[0, 1, 1, 2], [3, 4, 4, 5], [3, 4, 4, 5], [6, 7, 7, 8]])\n",
    "    .view(-1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(-1)\n",
    "    .contiguous()\n",
    ")\n",
    "\n",
    "print(\"Images\")\n",
    "print(f\"'b (h w) c' = {tuple(windows_batching.shape)}\")\n",
    "print(f\"'b (h w) c' = {tuple(windows_shifting.shape)}\")\n",
    "\n",
    "windows_shifting_rolled = windows_shifting.view(1, h, w, 1).roll((-hm // 2, -wm // 2), dims=(1, 2)).view(1, -1, 1)\n",
    "\n",
    "windows_batching_windowed = einops.rearrange(\n",
    "    windows_batching, \"b (hdm hm wdm wm) c -> (b hdm wdm) hm wm c\", hdm=hdm, hm=hm, wdm=wdm, wm=wm\n",
    ")\n",
    "windows_shifting_windowed = einops.rearrange(\n",
    "    windows_shifting_rolled, \"b (hdm hm wdm wm) c -> (b hdm wdm) hm wm c\", hdm=hdm, hm=hm, wdm=wdm, wm=wm\n",
    ")\n",
    "\n",
    "print(\"Windows\")\n",
    "print(f\"'(b hdm wdm) hm wm c' = {tuple(windows_batching_windowed.shape)}\")\n",
    "print(f\"'(b hdm wdm) hm wm c' = {tuple(windows_shifting_windowed.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (12, 5)\n",
    "fig = plt.figure(constrained_layout=True, figsize=figsize, dpi=dpi)\n",
    "fig.suptitle(\"Window batching and shifting\", fontsize=fontsize_figure)\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "ax.imshow(windows_batching.squeeze(0).squeeze(-1).view(h, w).detach().numpy())\n",
    "ax.set_title(\"Ordinary image\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(r\"$width$\")\n",
    "ax.set_ylabel(r\"$height$\")\n",
    "ax.axvline(hm - 0.5, color=\"#fff\", linewidth=2)\n",
    "ax.axhline(wm - 0.5, color=\"#fff\", linewidth=2)\n",
    "for i in range(1, hdm + 2):\n",
    "    ax.axhline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "for i in range(1, wdm + 2):\n",
    "    ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "ax.imshow(windows_batching_windowed.squeeze(-1).view(-1, w).detach().numpy())\n",
    "ax.set_title(\"Batched ordinary image\")\n",
    "ax.set_xticks(range(0, hm * wm))\n",
    "ax.set_yticks(range(0, hdm * wdm))\n",
    "ax.set_xlabel(\"Element Id\")\n",
    "ax.set_ylabel(\"Batch\")\n",
    "for i in range(0, hm * wm):\n",
    "    ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "for i in range(0, hdm * wdm):\n",
    "    ax.axhline(i - 0.5, color=\"#fff\", linewidth=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=figsize, dpi=dpi)\n",
    "fig.suptitle(\"Window batching and shifting\", fontsize=fontsize_figure)\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "ax.imshow(windows_shifting.squeeze(0).squeeze(-1).view(h, w).detach().numpy())\n",
    "ax.set_title(\"Pre-shifted image\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(r\"$width$\")\n",
    "ax.set_ylabel(r\"$height$\")\n",
    "ax.axvline(hm - 0.5, color=\"#fff\", linewidth=2)\n",
    "ax.axhline(wm - 0.5, color=\"#fff\", linewidth=2)\n",
    "for i in range(1, hdm + 2):\n",
    "    ax.axhline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "for i in range(1, wdm + 2):\n",
    "    ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "ax.imshow(windows_shifting_rolled.squeeze(0).squeeze(-1).view(h, w).detach().numpy())\n",
    "ax.set_title(\"Shifted image\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(r\"$width$\")\n",
    "ax.set_ylabel(r\"$height$\")\n",
    "ax.axvline(hm - 0.5, color=\"#fff\", linewidth=2)\n",
    "ax.axhline(wm - 0.5, color=\"#fff\", linewidth=2)\n",
    "for i in range(1, hdm + 2):\n",
    "    ax.axhline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "for i in range(1, wdm + 2):\n",
    "    ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "ax.imshow(windows_shifting_windowed.squeeze(-1).view(-1, w).detach().numpy())\n",
    "ax.set_title(\"Batched shifted image\")\n",
    "ax.set_xticks(range(0, hm * wm))\n",
    "ax.set_yticks(range(0, hdm * wdm))\n",
    "ax.set_xlabel(\"Element Id\")\n",
    "ax.set_ylabel(\"Batch\")\n",
    "for i in range(0, hm * wm):\n",
    "    ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "for i in range(0, hdm * wdm):\n",
    "    ax.axhline(i - 0.5, color=\"#fff\", linewidth=2)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters of a layer\n",
    "h, w = (4, 4)\n",
    "hm, wm = (2, 2)\n",
    "hsm, wsm = (hm // 2, wm // 2)\n",
    "c = 3 or 1\n",
    "c_embed = 12\n",
    "hdm, wdm = h // hm, w // wm\n",
    "\n",
    "rearrange_map = elt.Rearrange(\"b (hdm hm) (wdm wm) c -> (b hdm wdm) (hm wm) c\", hdm=hdm, wdm=wdm, hm=hm, wm=wm)\n",
    "\n",
    "# 1. Create a map with unique ids, where adjacent elements in a window\n",
    "#    share the same id. Each element in an image or volume gets an id assigned.\n",
    "id_map = torch.zeros((1, h, w, 1))\n",
    "\n",
    "# The first slice is the top left corner of a rolled image, there all elements\n",
    "# share the same id because they are always ajacent to each other. We slice up\n",
    "# to window size. The next slice is the right side where we split the elements\n",
    "# into two groups. The last slice is the bottom side where we split the elements\n",
    "# into two groups as well. The bottom right corner has four groups because we\n",
    "# splitted right and bottom.\n",
    "h_slices = [(0, -hm), (-hm, -hsm), (-hsm, None)]\n",
    "w_slices = [(0, -wm), (-wm, -wsm), (-wsm, None)]\n",
    "\n",
    "cnt = 0\n",
    "for h_start, h_stop in h_slices:\n",
    "    for w_start, w_stop in w_slices:\n",
    "        id_map[:, h_start:h_stop, w_start:w_stop, :] = cnt\n",
    "        cnt += 1\n",
    "\n",
    "print(\"ID Map\")\n",
    "print(f\"'1 h w 1' = {tuple(id_map.shape)}\")\n",
    "\n",
    "# 2. Rearrange map to batched windows\n",
    "id_windows = rearrange_map(id_map).squeeze(-1)  # (b hdm wdm) (hm wm)\n",
    "\n",
    "print(\"ID Windows\")\n",
    "print(f\"'(b hdm wdm) (hm wm)' = {tuple(id_windows.shape)}\")\n",
    "\n",
    "# 3. Attention mask for each window depending on its position in the batch. This\n",
    "#    part is very clever as one broadcasts the subtraction for each window to\n",
    "#    itself. The same id yields a zero (attention) and a different id yields any\n",
    "#    non-zero number (no attention).\n",
    "id_diff_windows = id_windows.unsqueeze(1) - id_windows.unsqueeze(2)\n",
    "mask = id_diff_windows != 0\n",
    "\n",
    "print(\"Mask\")\n",
    "print(f\"'(b hdm wdm) (hm wm) (hm wm)' = {tuple(mask.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (12, 5)\n",
    "fig = plt.figure(constrained_layout=True, figsize=figsize, dpi=dpi)\n",
    "fig.suptitle(\"Attention masking\", fontsize=fontsize_figure)\n",
    "gs = fig.add_gridspec(2, hdm * wdm)\n",
    "\n",
    "for i in range(hdm * wdm):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(id_windows[i].view(1, hm * wm).detach().numpy())\n",
    "    ax.set_title(f\"Window {i}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for j in range(hm * wm):\n",
    "        ax.axvline(j - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "        text = ax.text(j, 0, f\"{int(id_windows[i][j])}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "        text.set_backgroundcolor(\"white\")\n",
    "\n",
    "\n",
    "for i in range(hdm * wdm):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    ax.imshow(mask[i].detach().numpy())\n",
    "    ax.set_title(f\"Mask {i}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for i in range(hm * wm):\n",
    "        ax.axvline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "        ax.axhline(i - 0.5, color=\"#fff\", linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative positional embedding bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters of a layer\n",
    "h, w = (4, 4)\n",
    "hm, wm = (2, 2)\n",
    "c = 3 or 1\n",
    "c_embed = 12\n",
    "hdm, wdm = h // hm, w // wm\n",
    "num_heads = 3\n",
    "\n",
    "# 1. Embedding table for relative distances\n",
    "#    The maximum relative distance an element can have in a window is the widnow\n",
    "#    size minus one. Accordingly, to account positive and negative distances, we\n",
    "#    need to double the maximum distance and add one for the zero distance. This\n",
    "#    is the number of unique distances an element can have in a window. For each\n",
    "#    head, we want to have a unique embedding (bias). For instance, if we would\n",
    "#    context-aware embeddings then the size would be the query-key dimension\n",
    "#    (d_k).\n",
    "max_distance = (hm - 1, wm - 1)\n",
    "embedding_table = nn.Embedding(sum(2 * d + 1 for d in max_distance), num_heads)\n",
    "\n",
    "# 2. Offsets\n",
    "#    I decided to have a bias for each dimension too. Meaning, the number of\n",
    "#    embeddings is twice as large. Later, to get the correct indices, we need to\n",
    "#    add the offsets to the relative distance (indices).\n",
    "offsets = [0] + list(itertools.accumulate((2 * d + 1 for d in max_distance[:-1])))\n",
    "\n",
    "# 3. Absolute distances\n",
    "#    The absolute distances are the same for each window and can be precomputed.\n",
    "#    Note that we flatten the tensors as these are the relative distances for\n",
    "#    each element in a sequence (window is linearised).\n",
    "#    \n",
    "#    >>> h_abs_dist = [     >>> w_abs_dist = [\n",
    "#    >>>     [0, 0],        >>>     [0, 1],\n",
    "#    >>>     [1, 1],        >>>     [0, 1],\n",
    "#    >>> ]                  >>> ]\n",
    "#\n",
    "#    >>> h_abs_dist.flatten() = [0, 0, 1, 1]\n",
    "#    >>> w_abs_dist.flatten() = [0, 1, 0, 1]\n",
    "#    \n",
    "h_abs_dist = torch.arange(hm)\n",
    "w_abs_dist = torch.arange(wm)\n",
    "h_abs_dist = einops.repeat(h_abs_dist, \"p -> p wm\", wm=hm).flatten()\n",
    "w_abs_dist = einops.repeat(w_abs_dist, \"p -> hm p\", hm=wm).flatten()\n",
    "\n",
    "# 4. Relative distances\n",
    "#    We rearrange the tensors to broadcast the substraction (see attention mask)\n",
    "#    to get the relative distances for each element in a sequence.\n",
    "#\n",
    "#    >>> h_rel_dist = [         >>> w_rel_dist = [\n",
    "#    >>>     [ 0, 0, 1, 1],     >>>     [ 0, 1,  0, 1],\n",
    "#    >>>     [ 0, 0, 1, 1],     >>>     [-1, 0, -1, 0],\n",
    "#    >>>     [-1,-1, 0, 0],     >>>     [ 0, 1,  0, 1],\n",
    "#    >>>     [-1,-1, 0, 0],     >>>     [-1, 0, -1, 0],\n",
    "#    >>> ]                      >>> ]\n",
    "h_rel_dist = h_abs_dist.unsqueeze(0) - h_abs_dist.unsqueeze(1)\n",
    "w_rel_dist = w_abs_dist.unsqueeze(0) - w_abs_dist.unsqueeze(1)\n",
    "\n",
    "# 5. Indices\n",
    "#    To get valid indices for the embedding table, we need to clamp and shift \n",
    "#    the relative distances. The clamping is necessary to avoid out-of-bounds\n",
    "#    errors. The shifting is necessary to get positive indices. Remmeber, the\n",
    "#    embedding table has biases for both axis, therefore, we need to add the\n",
    "#    offsets to the indices.\n",
    "h_idx = torch.clamp(h_rel_dist, -max_distance[0], max_distance[0]) + max_distance[0] + offsets[0]\n",
    "w_idx = torch.clamp(w_rel_dist, -max_distance[1], max_distance[1]) + max_distance[1] + offsets[1]\n",
    "\n",
    "indices = torch.stack([h_idx, w_idx])\n",
    "biases = embedding_table(indices)\n",
    "embed = biases.sum(0)\n",
    "\n",
    "print(\"Paramters\")\n",
    "print(f\"'2 (hm wm) (hm wm)' = {tuple(indices.shape)}\")\n",
    "print(f\"'i nh' = {tuple(embedding_table.weight.shape)}\")\n",
    "print(f\"'2 (hm wm) (hm wm) nh' = {tuple(biases.shape)}\")\n",
    "print(f\"'(hm wm) (hm wm) nh' = {tuple(embed.shape)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
