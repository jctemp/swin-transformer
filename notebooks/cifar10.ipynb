{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as callbacks\n",
    "import lightning.pytorch.loggers as loggers\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "BASE_DIR = \"./notebooks/cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SwinTransformer2D, SwinTransformerConfig2D, PatchMode, RelativePositionalEmeddingMode\n",
    "from notebooks.cifar10.reference import SwinTransformerReference2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 1\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(root=f\"{BASE_DIR}/.data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=f\"{BASE_DIR}/.data\", train=False, download=True, transform=transform_test)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerClf(L.LightningModule):\n",
    "    def __init__(self, model, name, max_epochs=100, steps_per_epoch=300, learning_rate=3e-4, weight_decay=0.05):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.name = name\n",
    "        if isinstance(model, SwinTransformerConfig2D):\n",
    "            self.config = model \n",
    "            self.model = SwinTransformer2D(model)\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.save_hyperparameters(ignore=\"model\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"train_acc\": acc},\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": loss, \"val_acc\": acc},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = (y == y_hat.argmax(dim=1)).float().mean()\n",
    "        self.log_dict(\n",
    "            {\"test_loss\": loss, \"test_acc\": acc},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.learning_rate, epochs=self.max_epochs, steps_per_epoch=self.steps_per_epoch\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.swin = SwinTransformer2D(config)\n",
    "        self.norm = nn.LayerNorm(self.swin.out_channels[-1])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.fc = nn.Linear(self.swin.out_channels[-1], 10)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.swin(x)\n",
    "        x = out[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding_none = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=([(4, 4)] * 3) + [(2, 2)],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "        rpe_mode=RelativePositionalEmeddingMode.NONE,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_embedding_bias = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=([(4, 4)] * 3) + [(2, 2)],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "        rpe_mode=RelativePositionalEmeddingMode.BIAS,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_embedding_context = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=([(4, 4)] * 3) + [(2, 2)],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "        rpe_mode=RelativePositionalEmeddingMode.CONTEXT,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_merge_convolution = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=([(4, 4)] * 3) + [(2, 2)],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "        patch_mode=[PatchMode.CONCATENATE] * 4,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_merge_convolution = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=([(4, 4)] * 3) + [(2, 2)],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "        patch_mode=[PatchMode.CONVOLUTION] * 4,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_odd_windows = SwinModel(\n",
    "    SwinTransformerConfig2D(\n",
    "        input_size=(32, 32),\n",
    "        in_channels=3,\n",
    "        embed_dim=32,\n",
    "        num_blocks=[2, 4, 4, 2],\n",
    "        patch_window_size=[(2, 2)] * 4,\n",
    "        block_window_size=[(3, 3)] * 4,\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        drop_path=0.1,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_reference = SwinTransformerReference2D(\n",
    "    num_classes=10,\n",
    "    img_size=32,\n",
    "    in_chans=3,\n",
    "    embed_dim=32,\n",
    "    depths=[2, 4, 4, 2],\n",
    "    patch_size=2,\n",
    "    window_size=4,\n",
    "    num_heads=[2, 4, 8, 16],\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"odd_windows\": model_odd_windows,\n",
    "    \"embedding_none\": model_embedding_none,\n",
    "    \"embedding_bias\": model_embedding_bias,\n",
    "    \"embedding_context\": model_embedding_context,\n",
    "    \"merge_concatenate\": model_merge_convolution,\n",
    "    \"merge_convolution\": model_merge_convolution,\n",
    "    \"reference\": model_reference,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    SwinTransformerClf(model, name, num_epochs, len(train_loader), learning_rate, weight_decay)\n",
    "    for name, model in models.items()\n",
    "]\n",
    "for m in model_list:\n",
    "    print(summary(m, (batch_size, 3, 32, 32))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_list:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    csv_logger = loggers.CSVLogger(f\"{BASE_DIR}/logs\", name=model.name)\n",
    "    learning_rate_monitor = callbacks.LearningRateMonitor(logging_interval=\"epoch\")\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_acc\",\n",
    "        dirpath=f\"{BASE_DIR}/checkpoints\",\n",
    "        filename=model.name,\n",
    "        save_top_k=1,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        logger=csv_logger,\n",
    "        callbacks=[learning_rate_monitor, model_checkpoint],\n",
    "        gradient_clip_val=1.0,\n",
    "        precision=\"16-mixed\",\n",
    "    )\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    trainer.test(model=model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f\"{BASE_DIR}/checkpoints\"\n",
    "log_path = f\"{BASE_DIR}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logs = [os.path.join(log_path, f) for f in os.listdir(log_path)]\n",
    "model_versions = [[os.path.join(f, v) for v in os.listdir(f)] for f in model_logs]\n",
    "model_versions_latest = [max(v, key=os.path.getctime) for v in model_versions]\n",
    "model_csvs = [os.path.join(v, \"metrics.csv\") for v in model_versions_latest]\n",
    "model_hparams = [os.path.join(v, \"hparams.yaml\") for v in model_versions_latest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_files = os.listdir(ckpt_path)\n",
    "ckpt_files = [f for f in ckpt_files if f.endswith(\".ckpt\")]\n",
    "\n",
    "for ckpt in ckpt_files:\n",
    "    model_name = ckpt.split(\".\")[0]\n",
    "    model = SwinModel(None)\n",
    "    model = SwinTransformerClf.load_from_checkpoint(f\"{ckpt_path}/{ckpt}\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 12), dpi=100)\n",
    "fig.suptitle(\"Training, Validation, and Test Accuracy over Epochs\")\n",
    "\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax_acc = fig.add_subplot(gs[0, 0])\n",
    "ax_loss = fig.add_subplot(gs[0, 1])\n",
    "ax_test = fig.add_subplot(gs[1, :])\n",
    "\n",
    "for i, (csv, hparams) in enumerate(zip(model_csvs, model_hparams)):\n",
    "    with open(hparams, \"r\") as file:\n",
    "        hparams = yaml.safe_load(file)\n",
    "\n",
    "    print(\"Hyperparameters:\")\n",
    "    for key, value in hparams.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "    df = pd.read_csv(csv)\n",
    "    df[\"epoch\"] = df[\"step\"] // hparams[\"steps_per_epoch\"]\n",
    "    col = list(mcolors.BASE_COLORS.keys())[i]\n",
    "\n",
    "    train_data = df[df[\"train_acc_epoch\"].notna()]\n",
    "    val_data = df[df[\"val_acc\"].notna()]\n",
    "    ax_acc.plot(\n",
    "        train_data[\"epoch\"],\n",
    "        train_data[\"train_acc_epoch\"],\n",
    "        label=f\"Training Accuracy {hparams['name']}\",\n",
    "        color=col,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax_acc.plot(\n",
    "        val_data[\"epoch\"],\n",
    "        val_data[\"val_acc\"],\n",
    "        label=f\"Validation Accuracy {hparams['name']}\",\n",
    "        color=col,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "    ax_acc.set_xlabel(\"Epoch\")\n",
    "    ax_acc.set_ylabel(\"Accuracy\")\n",
    "    ax_acc.legend()\n",
    "    ax_acc.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    train_data = df[df[\"train_loss_epoch\"].notna()]\n",
    "    val_data = df[df[\"val_loss\"].notna()]\n",
    "    ax_loss.plot(\n",
    "        train_data[\"epoch\"],\n",
    "        train_data[\"train_loss_epoch\"],\n",
    "        label=f\"Training Loss {hparams['name']}\",\n",
    "        color=col,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax_loss.plot(\n",
    "        val_data[\"epoch\"],\n",
    "        val_data[\"val_loss\"],\n",
    "        label=f\"Validation Loss {hparams['name']}\",\n",
    "        color=col,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "    ax_loss.set_xlabel(\"Epoch\")\n",
    "    ax_loss.set_ylabel(\"Loss\")\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    test_data = df[df[\"test_acc\"].notna()]\n",
    "\n",
    "    ax_test.barh(i, test_data[\"test_acc\"], label=f\"Test Accuracy {hparams['name']}\")\n",
    "    ax_test.set_xlabel(\"Epoch\")\n",
    "    ax_test.set_ylabel(\"Accuracy\")\n",
    "    ax_test.legend()\n",
    "    ax_test.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    ax_test.axvline(test_data[\"test_acc\"].max(), color=\"black\", linestyle=\"--\")\n",
    "    ax_test.text(\n",
    "        test_data[\"test_acc\"].max() + 0.01,\n",
    "        i,\n",
    "        f\"{test_data['test_acc'].max():.2f}\",\n",
    "        va=\"center\",\n",
    "        ha=\"left\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in model_list:\n",
    "    print(m.name)\n",
    "    attn_weights = m.model.swin.stages[-1].blocks[-1].attn.attn_weights.mean(dim=1)\n",
    "    count = attn_weights.shape[0]\n",
    "    split = int(math.sqrt(count))\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(12, 12), dpi=100)\n",
    "    fig.suptitle(\"Attention Weights\")\n",
    "\n",
    "    gs = fig.add_gridspec(split, split)\n",
    "\n",
    "    for i in range(count):\n",
    "        ax = fig.add_subplot(gs[i // split, i % split])\n",
    "        ax.imshow(attn_weights[i].detach().cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
